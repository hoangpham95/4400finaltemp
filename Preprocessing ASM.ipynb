{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.rdd import RDD, PipelinedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_PREF = [\n",
    "    'HEADER:', '.text:', '.Pav:', '.idata', '.data', '.rdata', '.bss', '.edata:',\n",
    "    '.rsrc:', '.tls', '.reloc:'\n",
    "]\n",
    "\n",
    "OP_INSTR = [\n",
    "    'jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add',\n",
    "    'imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb'\n",
    "]\n",
    "\n",
    "KEY = ['.dll', 'std::', ':dword']\n",
    "\n",
    "# important keyword to interact with stack and manipulate memories\n",
    "MEM_KW = ['FUNCTION', 'call'] # memcpy_s and memmove_s usually comes after call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessASM:\n",
    "    def __init__(self, fdir, minApp=30):\n",
    "        self.fdir = fdir\n",
    "        self.minApp = minApp\n",
    "    \n",
    "    def genToken(self, file):\n",
    "        asm = os.path.join(self.fdir, file + '.asm')\n",
    "        with open(asm, 'r', encoding='ISO-8859-1') as asmFile:\n",
    "            tokens = re.sub(r'\\t|\\n|\\r', ' ', asmFile.read()).split()\n",
    "        \n",
    "        filtered = []\n",
    "        opList = []\n",
    "        \n",
    "        for i in range(1, len(tokens) - 1):\n",
    "            if tokens[i] in OP_INSTR:\n",
    "                filtered.append(tokens[i])\n",
    "                opList.append(tokens[i])\n",
    "            \n",
    "            filtered += [p for p in SECTION_PREF if p in tokens[i]]\n",
    "            filtered += [k for k in KEY if k in tokens[i]]\n",
    "            filtered += [tokens[i] + ' ' + tokens[i + 1] for k in MEM_KW if k == tokens[i]]\n",
    "            \n",
    "            # memory and function call\n",
    "            if tokens[i] == '__stdcall':\n",
    "                bigram = tokens[i] + ' ' + tokens[i + 1].partition('(')[0]\n",
    "                filtered.append(bigram)\n",
    "                filtered.append(tokens[i - 1])\n",
    "            \n",
    "            # define bytes\n",
    "            if tokens[i] == 'db' and tokens[i + 1][0] == \"'\":\n",
    "                filtered.append(tokens[i] + ' ' + tokens[i + 1])\n",
    "        \n",
    "        return (file, filtered)\n",
    "    \n",
    "    def getFileSize(file):\n",
    "        return (file, os.stat(os.path.join(self.fdir, file + '.asm')).st_size)\n",
    "    \n",
    "    def genBagWords(self, row):\n",
    "        sparse = {}\n",
    "        df = {}\n",
    "        \n",
    "        for w in row[1]:\n",
    "            if w in self.dict:\n",
    "                if self.dict[w] not in sparse:\n",
    "                    sparse[self.dict[w]] = 0\n",
    "                sparse[self.dict[w]] += 1\n",
    "        \n",
    "        tf = SparseVector(len(self.dict), sparse)\n",
    "        return (row[0], tf)\n",
    "                \n",
    "    \n",
    "    def process(self, X_RDD, y_RDD=None, train=True):\n",
    "        X = X_RDD.map(self.genToken).cache()\n",
    "        X_sz = X_RDD.map(self.getFileSize).cache()\n",
    "        \n",
    "        if train:\n",
    "            self.dict = X.map(lambda r: r[1]) \\\n",
    "                         .flatMap(lambda w: w) \\\n",
    "                         .map(lambda w: (w, 1)) \\\n",
    "                         .reduceByKey(lambda acc, w: acc + w) \\\n",
    "                         .filter(lambda x: x[1] >= self.minApp) \\\n",
    "                         .collectAsMap()\n",
    "            \n",
    "            \n",
    "            self.dict = dict(zip(self.dict, range(len(self.dict))))\n",
    "        # bag of words\n",
    "        X = X.map(self.genBagWords)\n",
    "        \n",
    "        if y_RDD:\n",
    "            X = X.zipWithIndex().map(lambda r: (r[1], r[0]))\n",
    "            y = y_RDD.zipWithIndex().map(lambda r: (r[1], r[0]))\n",
    "            \n",
    "            # x: (idx,((hash,features),label)\n",
    "            data = X.join(y).map(lambda x: (x[1][0][0], x[1][0][1], x[1][1]))\n",
    "            schema = StructType([\n",
    "                StructField('hash', StringType(), True),\n",
    "                StructField('features', VectorUDT(), True),\n",
    "                StructField('label', StringType(), True)\n",
    "            ])\n",
    "            data = data.toDF(schema)\n",
    "            data = data.withColumn('label', data.label.cast(BooleanType()))\n",
    "        else:\n",
    "            schema = StructType([\n",
    "                StructField('hash', StringType(), True),\n",
    "                StructField('features', VectorUDT(), True)\n",
    "            ])\n",
    "            data = X.toDF(schema)\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    sparkRaw = SparkSession.builder.appName('MSFT_MAL').getOrCreate()\n",
    "    sc = sparkRaw.sparkContext\n",
    "    \n",
    "    X_file = ['0ACDbR5M3ZhBJajygTuf', '0A32eTdBKayjCWhZqDOQ']\n",
    "    y_file = [1, 0]\n",
    "    \n",
    "    X = sc.parallelize(X_file)\n",
    "    y = sc.parallelize(y_file)\n",
    "    \n",
    "    pipeline = PreprocessASM('data/', 30)\n",
    "    data = pipeline.process(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.save(\"/tmp/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
